{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.282      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 38.4       |\n",
      "|    reward             | 0.11025473 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.09       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -55.4      |\n",
      "|    reward             | -1.3854662 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.56       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 81       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0.0042   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -339     |\n",
      "|    reward             | 3.391866 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 70.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 86       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -70.7    |\n",
      "|    reward             | 6.332755 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 4.11     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 87        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 380       |\n",
      "|    reward             | -4.429727 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 104       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 88          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 33          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.00171    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 193         |\n",
      "|    reward             | 0.087448716 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 22.7        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -65.8      |\n",
      "|    reward             | -2.7318516 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.78       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -122       |\n",
      "|    reward             | -3.5182903 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 81.2       |\n",
      "|    reward             | -0.7624608 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.53       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 90          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 55          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 90.2        |\n",
      "|    reward             | -0.76001424 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 5.31        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -98.2     |\n",
      "|    reward             | 0.8892505 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.8       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 92         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -0.0536    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -72.2      |\n",
      "|    reward             | 0.26658863 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.88       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0.0217     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -1.81      |\n",
      "|    reward             | -2.2109706 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 130       |\n",
      "|    reward             | 0.8835194 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 12.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -186      |\n",
      "|    reward             | 1.2555037 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 45.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 95         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 251        |\n",
      "|    reward             | 0.24808238 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 37         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -19.5     |\n",
      "|    reward             | 7.7073183 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.66      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0.000519   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -30.7      |\n",
      "|    reward             | 0.51203763 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.869      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 97          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 44          |\n",
      "|    reward             | 0.041149914 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.63        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 10.6      |\n",
      "|    reward             | 0.7703739 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.256     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 32.8      |\n",
      "|    reward             | 0.6852897 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.1       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 111        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | -32.5      |\n",
      "|    reward             | -7.1390004 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.56       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 99         |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -2.37e+03  |\n",
      "|    reward             | -10.550006 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.2e+03    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 99        |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0.0284    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 67.5      |\n",
      "|    reward             | 0.3746248 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.68      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 99         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -30.5      |\n",
      "|    reward             | -0.6775755 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.23       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 100       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 32.3      |\n",
      "|    reward             | 3.0196373 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 100        |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 134        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0.0648     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 48.5       |\n",
      "|    reward             | -1.8973987 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 100       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 308       |\n",
      "|    reward             | 2.8944852 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 57.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 101       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -3.58e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -117      |\n",
      "|    reward             | 1.8488793 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.88      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 101      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 3.98     |\n",
      "|    reward             | 0.260266 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 101         |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 152         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 6.14        |\n",
      "|    reward             | -0.18214704 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.58        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 101        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 157        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -98.9      |\n",
      "|    reward             | -1.9565057 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 19.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 101       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 162       |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | -199      |\n",
      "|    reward             | 1.7313249 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 32.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 101       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 166       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 120       |\n",
      "|    reward             | 5.4731264 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 16.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 102        |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 171        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -0.027     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 111        |\n",
      "|    reward             | 0.15176553 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.87       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 176       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -22.9     |\n",
      "|    reward             | 1.2557095 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 102        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 180        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | 112        |\n",
      "|    reward             | -1.9729638 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 8.67       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 185       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 120       |\n",
      "|    reward             | 0.6203998 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 8.83      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 190       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -217      |\n",
      "|    reward             | 1.0554643 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 29.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 194       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -314      |\n",
      "|    reward             | 4.7121725 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 54.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 102        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 199        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -7.29      |\n",
      "|    reward             | 0.08846258 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.762      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 204       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | -85.1     |\n",
      "|    reward             | 0.9444181 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.96      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 102      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 208      |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -134     |\n",
      "|    reward             | 4.770794 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 12.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 213       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 105       |\n",
      "|    reward             | 1.9008546 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 11.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 218        |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 206        |\n",
      "|    reward             | -1.3398763 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 37.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 222       |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 168       |\n",
      "|    reward             | 6.0184593 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 22.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 227       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -110      |\n",
      "|    reward             | 0.3417877 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 9.51      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 232       |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -108      |\n",
      "|    reward             | -0.976139 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 6.47      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 237        |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | -36.5      |\n",
      "|    reward             | 0.47295007 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.31       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 241        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 56.9       |\n",
      "|    reward             | -0.5308896 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 4.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 246        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | -110       |\n",
      "|    reward             | -1.9031373 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 17.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 251       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -480      |\n",
      "|    reward             | 3.9432917 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 147       |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3758073.56\n",
      "total_reward: 2758073.56\n",
      "total_cost: 5975.66\n",
      "total_trades: 49683\n",
      "Sharpe: 0.809\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 256       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -27.7     |\n",
      "|    reward             | 0.5097055 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.468     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 103         |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 260         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | 97.4        |\n",
      "|    reward             | -0.21185152 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 13.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 265       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0.00356   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -55.4     |\n",
      "|    reward             | 2.0259786 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 19.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 269        |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | -113       |\n",
      "|    reward             | -1.3711272 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 103         |\n",
      "|    iterations         | 5700        |\n",
      "|    time_elapsed       | 274         |\n",
      "|    total_timesteps    | 28500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5699        |\n",
      "|    policy_loss        | -72         |\n",
      "|    reward             | -0.98385024 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 7.47        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 279       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -0.126    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -34.8     |\n",
      "|    reward             | 1.0769047 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.4       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 103          |\n",
      "|    iterations         | 5900         |\n",
      "|    time_elapsed       | 283          |\n",
      "|    total_timesteps    | 29500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5899         |\n",
      "|    policy_loss        | -16.5        |\n",
      "|    reward             | -0.034104265 |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 0.287        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 103         |\n",
      "|    iterations         | 6000        |\n",
      "|    time_elapsed       | 288         |\n",
      "|    total_timesteps    | 30000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5999        |\n",
      "|    policy_loss        | 58          |\n",
      "|    reward             | -0.38149226 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 3.77        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 293        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | 35.5       |\n",
      "|    reward             | -1.8691139 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.74       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 297         |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | -9          |\n",
      "|    reward             | -0.48611382 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.04        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 302       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 41        |\n",
      "|    reward             | 3.1559389 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 5.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 307       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0.0421    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 49.2      |\n",
      "|    reward             | 0.8848122 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 311       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0.289     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 0.413     |\n",
      "|    reward             | -3.396548 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1         |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 316         |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -79.3       |\n",
      "|    reward             | -0.46146256 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 6.69        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 321       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0.00458   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -780      |\n",
      "|    reward             | -5.730236 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 382       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 325        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | -209       |\n",
      "|    reward             | 0.55269736 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 27.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 330        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 125        |\n",
      "|    reward             | -0.7730595 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 24.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 335        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 29.8       |\n",
      "|    reward             | 0.17269997 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.839      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 339       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 24.9      |\n",
      "|    reward             | 0.5368473 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 344        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -283       |\n",
      "|    reward             | 0.23633873 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 40         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 349        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 175        |\n",
      "|    reward             | -0.2561297 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 25.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 353       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -18       |\n",
      "|    reward             | -5.361117 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 6.32      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 358       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 95.7      |\n",
      "|    reward             | 1.2376721 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 6.68      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 363       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -102      |\n",
      "|    reward             | 1.2518629 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 5.92      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 369       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -53.2     |\n",
      "|    reward             | 1.7100748 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.17      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 375        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | -35.6      |\n",
      "|    reward             | -3.2164109 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.91       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 380       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 374       |\n",
      "|    reward             | 1.6105194 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 75.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 385       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -19.7     |\n",
      "|    reward             | 2.5263324 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.82      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 103      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 390      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -40.7    |\n",
      "|    reward             | 4.778373 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 11.6     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 395        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | 39.1       |\n",
      "|    reward             | 0.24881849 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 399        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 69.8       |\n",
      "|    reward             | -1.1919031 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.5        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 404        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -6.27      |\n",
      "|    reward             | -1.4026177 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.395      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 103         |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 409         |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -213        |\n",
      "|    reward             | -0.24122107 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 26.6        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 413       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | 48.2      |\n",
      "|    reward             | -3.835304 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 38.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 418        |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | 2.8        |\n",
      "|    reward             | 0.42422602 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.954      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 423       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -77.4     |\n",
      "|    reward             | 1.1890944 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 3.23      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 8900        |\n",
      "|    time_elapsed       | 427         |\n",
      "|    total_timesteps    | 44500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43         |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8899        |\n",
      "|    policy_loss        | 79          |\n",
      "|    reward             | -0.68389106 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 4.92        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 9000        |\n",
      "|    time_elapsed       | 432         |\n",
      "|    total_timesteps    | 45000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8999        |\n",
      "|    policy_loss        | 16.2        |\n",
      "|    reward             | -0.44394338 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 4.04        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 437       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -21.4     |\n",
      "|    reward             | 6.438101  |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.743     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 441       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | 121       |\n",
      "|    reward             | 1.0236892 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 10.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 446        |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | -0.29      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | -69        |\n",
      "|    reward             | 0.93197626 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 3.81       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 450        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | -0.0773    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 129        |\n",
      "|    reward             | 0.41273853 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 9.06       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 455        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 141        |\n",
      "|    reward             | 0.08912667 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 13.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 460        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -89.5      |\n",
      "|    reward             | 0.13666596 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 5.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 465        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 94.8       |\n",
      "|    reward             | 0.38808212 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 7.65       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 469       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 324       |\n",
      "|    reward             | 6.8032737 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 59.4      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 104          |\n",
      "|    iterations         | 9900         |\n",
      "|    time_elapsed       | 474          |\n",
      "|    total_timesteps    | 49500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -43.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9899         |\n",
      "|    policy_loss        | 30.1         |\n",
      "|    reward             | 0.0035519307 |\n",
      "|    std                | 1.07         |\n",
      "|    value_loss         | 0.718        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 10000       |\n",
      "|    time_elapsed       | 479         |\n",
      "|    total_timesteps    | 50000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9999        |\n",
      "|    policy_loss        | 28.1        |\n",
      "|    reward             | -0.43205363 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 1.01        |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cuda device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3990512.90\n",
      "total_reward: 2990512.90\n",
      "total_cost: 5286.30\n",
      "total_trades: 55707\n",
      "Sharpe: 0.800\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 77        |\n",
      "|    time_elapsed    | 148       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -49.5     |\n",
      "|    critic_loss     | 135       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 8679      |\n",
      "|    reward          | 4.3509665 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 73        |\n",
      "|    time_elapsed    | 313       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -29.5     |\n",
      "|    critic_loss     | 11.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 20251     |\n",
      "|    reward          | 4.3509665 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4038764.75\n",
      "total_reward: 3038764.75\n",
      "total_cost: 999.00\n",
      "total_trades: 46245\n",
      "Sharpe: 0.784\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 67        |\n",
      "|    time_elapsed    | 515       |\n",
      "|    total_timesteps | 34716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -20.2     |\n",
      "|    critic_loss     | 2.29      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 31823     |\n",
      "|    reward          | 4.3509665 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 67        |\n",
      "|    time_elapsed    | 690       |\n",
      "|    total_timesteps | 46288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -15.6     |\n",
      "|    critic_loss     | 1.53      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 43395     |\n",
      "|    reward          | 4.3509665 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 97          |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 21          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.020233056 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016614027 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00975    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.58        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    reward               | 1.1401734   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 9.79        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3830414.75\n",
      "total_reward: 2830414.75\n",
      "total_cost: 337624.52\n",
      "total_trades: 80519\n",
      "Sharpe: 0.689\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016332332 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00348    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    reward               | -1.5551344  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 61.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008956104 |\n",
      "|    clip_fraction        | 0.0753      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00582     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 141         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    reward               | 3.1444545   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 237         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014296137 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0516     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    reward               | 2.9111667   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 25          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028209247 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.00886    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | 2.1514475   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 66.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016963318 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.00439     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26          |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    reward               | 1.7487807   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 157         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017198525 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.0177     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    reward               | 1.2313813   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 31.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015560304 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00549    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    reward               | -0.05745077 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 69.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 199         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023841653 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.00486     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    reward               | 1.0090605   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 71.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017691765 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.00431     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.3        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | -2.7392414  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 50.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020781098 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.00696     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.54        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    reward               | -0.25397715 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 25.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020514071 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | -0.00208    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 76.2        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    reward               | 0.31476477  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 284         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019134384 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.00458     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 58.7        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | 0.040682994 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 94          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 303         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017354943 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | -0.0296     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.52        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    reward               | 3.7676249   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 23.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020209845 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | -0.00256    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    reward               | -0.42414075 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 43          |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5046854.87\n",
      "total_reward: 4046854.87\n",
      "total_cost: 337720.10\n",
      "total_trades: 80151\n",
      "Sharpe: 0.870\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 341         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021866392 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | -0.00338    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 60.2        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    reward               | 0.804032    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 75.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021577742 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | -0.0187     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39          |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    reward               | -1.0861422  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 132         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03598381  |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | -0.0245     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.39        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    reward               | -0.51048476 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 20.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022985812 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | -0.000124   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 56.9        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    reward               | -0.17416683 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 87.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 424         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025186807 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.00742     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    reward               | -7.5072427  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 53.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029560171 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | -0.00341    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    reward               | 2.710092    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 29.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03276337  |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.005       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 42.4        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    reward               | -0.15345418 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 101        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 483        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03130112 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.3      |\n",
      "|    explained_variance   | 0.00448    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 60.9       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    reward               | 7.858115   |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 60         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 501         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035679255 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.00272     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 67.9        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    reward               | -0.1196461  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 102        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 521        |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02294702 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.4      |\n",
      "|    explained_variance   | -0.00835   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 16.7       |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    reward               | 0.79509485 |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 29.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 539         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029297683 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.00436     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 55.4        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    reward               | -0.6547185  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 557         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030363379 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00645     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.2        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    reward               | -0.365066   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 76.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 575         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027431753 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0059      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.7        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | 4.1839476   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 23.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 592         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022260884 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0014      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.9        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    reward               | 0.4827492   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 66.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 104        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 609        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02043109 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | -0.00464   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 49.7       |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    reward               | 5.625035   |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 96.8       |\n",
      "----------------------------------------\n",
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4052945.55\n",
      "total_reward: 3052945.55\n",
      "total_cost: 313074.34\n",
      "total_trades: 77783\n",
      "Sharpe: 0.840\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 104          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 627          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.029359937  |\n",
      "|    clip_fraction        | 0.278        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.7        |\n",
      "|    explained_variance   | 0.00628      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 19.1         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00768     |\n",
      "|    reward               | -0.047850505 |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 37.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 645         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030758934 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | -0.0273     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    reward               | -0.53193337 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 54.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 662         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028932806 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.00686     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | 1.1589198   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 53.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 680         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027908195 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.00503     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48          |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00996    |\n",
      "|    reward               | 0.8199292   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 215         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 698         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040287144 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | -0.0248     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.48        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | -5.082193   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 17.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 716         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03675144  |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.00886     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 40.1        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | -0.51240253 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 63.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 734         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033948477 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | -0.00215    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    reward               | -7.0982575  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 751         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022807235 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.0271      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00597    |\n",
      "|    reward               | -3.5229821  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 106        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 769        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03216366 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.2      |\n",
      "|    explained_variance   | 0.00291    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 10.3       |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    reward               | -1.0320076 |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 77.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 787         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036064602 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.0112      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.6        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00544    |\n",
      "|    reward               | 0.38228673  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 804         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027661435 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | -0.0104     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00657    |\n",
      "|    reward               | -0.4453181  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 58.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 822         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027768198 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0153      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.35        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    reward               | -0.5950569  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 839         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030285668 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.0032      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 59          |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00738    |\n",
      "|    reward               | 0.2528765   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 93.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 857         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029212791 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | -0.00514    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 64.1        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    reward               | 0.2864157   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6223713.95\n",
      "total_reward: 5223713.95\n",
      "total_cost: 325461.83\n",
      "total_trades: 78189\n",
      "Sharpe: 0.999\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 874        |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04643644 |\n",
      "|    clip_fraction        | 0.334      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.6      |\n",
      "|    explained_variance   | -0.00391   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 23.2       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.00864   |\n",
      "|    reward               | -5.471007  |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 44.9       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 892        |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02952569 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.7      |\n",
      "|    explained_variance   | -0.0036    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 112        |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    reward               | 1.1923963  |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 133        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 909         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026492959 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | -0.00489    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 33.9        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    reward               | 12.741209   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 86.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 927         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029927118 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | -0.00295    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.6        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00958    |\n",
      "|    reward               | -1.3748448  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 944        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03208095 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.9      |\n",
      "|    explained_variance   | 0.00985    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 29.8       |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    reward               | 0.42778212 |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 50.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 962         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026934743 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0101      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 47          |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    reward               | -0.12136642 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 233         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 979         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038600534 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.00029     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 115         |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00986    |\n",
      "|    reward               | -3.4217103  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 997         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036651038 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.0288      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.7        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00926    |\n",
      "|    reward               | 3.7280262   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 41.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 1015        |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028470214 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.00968     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00552    |\n",
      "|    reward               | 1.2410479   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 1032        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026078358 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.0179      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 86.4        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00869    |\n",
      "|    reward               | 0.5561161   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 168         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 1049        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024531223 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.0175      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00437    |\n",
      "|    reward               | 1.9761248   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 50.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 1067        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029285105 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.0076      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 38.1        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | 0.31480572  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 1084        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027524665 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.014       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 61.6        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    reward               | 0.67348945  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 1102        |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035444025 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | -0.00158    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 70.3        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00113    |\n",
      "|    reward               | -1.8806355  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 155         |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4367320.71\n",
      "total_reward: 3367320.71\n",
      "total_cost: 299417.35\n",
      "total_trades: 76503\n",
      "Sharpe: 0.796\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 1120       |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03247026 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.5      |\n",
      "|    explained_variance   | 0.0697     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 16.7       |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    reward               | 0.21105283 |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 30.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 1138        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025711142 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.0289      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 44.4        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00982    |\n",
      "|    reward               | 0.86830705  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 1157       |\n",
      "|    total_timesteps      | 126976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03209849 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.7      |\n",
      "|    explained_variance   | 0.0222     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 86.6       |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.00477   |\n",
      "|    reward               | 6.3152614  |\n",
      "|    std                  | 1.13       |\n",
      "|    value_loss           | 152        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 1174        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021212902 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.00386     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.6        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    reward               | 5.6451497   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 39.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 1192        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028615076 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.0273      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 63.5        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    reward               | -0.22170344 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 1209        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025787722 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.0259      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 61.7        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    reward               | -1.4330577  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 154         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 1227        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044934466 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.00543     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 73.7        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.00588     |\n",
      "|    reward               | 2.7523956   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 167         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 1244        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042157225 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    reward               | -1.7714602  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 29.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 1262        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028857064 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.0473      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.5        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00981    |\n",
      "|    reward               | 0.10262209  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 1280        |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035281073 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.0207      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 61.6        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    reward               | 1.894777    |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 1297       |\n",
      "|    total_timesteps      | 143360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04755744 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.1      |\n",
      "|    explained_variance   | 0.116      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 25.7       |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.000433  |\n",
      "|    reward               | 1.0903006  |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 46.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 1315        |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024918608 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | -0.00579    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 55.8        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 0.8389404   |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 154         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 1333        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029028082 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.00158     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 126         |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00434    |\n",
      "|    reward               | -17.191538  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 228         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 1350        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022142101 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.0602      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 29.1        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    reward               | -0.85172176 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 55.7        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5952479.32\n",
      "total_reward: 4952479.32\n",
      "total_cost: 277112.61\n",
      "total_trades: 75548\n",
      "Sharpe: 0.932\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 1368        |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027287846 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.0856      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.5        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -2.0646653  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 50.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 1386       |\n",
      "|    total_timesteps      | 153600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03027108 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.3      |\n",
      "|    explained_variance   | 0.0266     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 60.2       |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    reward               | 1.0992491  |\n",
      "|    std                  | 1.16       |\n",
      "|    value_loss           | 166        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 1407        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024960503 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.0344      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 65.7        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    reward               | -2.112159   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 77         |\n",
      "|    time_elapsed         | 1427       |\n",
      "|    total_timesteps      | 157696     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04468357 |\n",
      "|    clip_fraction        | 0.414      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.4      |\n",
      "|    explained_variance   | 0.0405     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 14.3       |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | 0.0047     |\n",
      "|    reward               | 0.8548301  |\n",
      "|    std                  | 1.16       |\n",
      "|    value_loss           | 28.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 1447        |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021239018 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.0483      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 43.2        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00619    |\n",
      "|    reward               | 2.9244356   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 99.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 1467        |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021285415 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.0354      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 73.4        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0058     |\n",
      "|    reward               | -4.517798   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 152         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1488        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035121735 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.6       |\n",
      "|    explained_variance   | 0.057       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34.5        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    reward               | -0.4839865  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 63.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 1507        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035313547 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.6       |\n",
      "|    explained_variance   | 0.00167     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 56.4        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00533    |\n",
      "|    reward               | -0.49533653 |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 130         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 1528       |\n",
      "|    total_timesteps      | 167936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03232641 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.7      |\n",
      "|    explained_variance   | 0.0372     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 60.4       |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.00415   |\n",
      "|    reward               | 0.37259644 |\n",
      "|    std                  | 1.17       |\n",
      "|    value_loss           | 98.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 1548        |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03147866  |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.0301      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 45.9        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    reward               | -0.12214053 |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 183         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 1568        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019143187 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    reward               | 0.55154085  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 29.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1588        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030926457 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.8       |\n",
      "|    explained_variance   | 0.0515      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | 0.00125     |\n",
      "|    reward               | 0.014717502 |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1608        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026563242 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.9       |\n",
      "|    explained_variance   | 0.0336      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 54.4        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.00268     |\n",
      "|    reward               | -1.2039502  |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 130         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1628        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029672872 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.9       |\n",
      "|    explained_variance   | 0.0423      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.8        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    reward               | 2.139257    |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 43          |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5440252.47\n",
      "total_reward: 4440252.47\n",
      "total_cost: 228788.22\n",
      "total_trades: 72380\n",
      "Sharpe: 0.917\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1648        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023213124 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46         |\n",
      "|    explained_variance   | 0.0638      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00368    |\n",
      "|    reward               | -1.7594525  |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1668        |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034683395 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46         |\n",
      "|    explained_variance   | -0.0018     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 45.6        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.000302   |\n",
      "|    reward               | -0.09267654 |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 158         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1689        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020717867 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.1       |\n",
      "|    explained_variance   | 0.0586      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 70.1        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00456    |\n",
      "|    reward               | -0.87722206 |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 1711       |\n",
      "|    total_timesteps      | 186368     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05710158 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.2      |\n",
      "|    explained_variance   | 0.043      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 14.8       |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | 0.0046     |\n",
      "|    reward               | -1.1827924 |\n",
      "|    std                  | 1.19       |\n",
      "|    value_loss           | 21.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1731        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028326593 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.3       |\n",
      "|    explained_variance   | 0.0249      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 32.1        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00309    |\n",
      "|    reward               | -1.9556996  |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 97.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1752        |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035645988 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.3       |\n",
      "|    explained_variance   | 0.0119      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 45.2        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00524    |\n",
      "|    reward               | -0.11332548 |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 148         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 108          |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 1772         |\n",
      "|    total_timesteps      | 192512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.03274678   |\n",
      "|    clip_fraction        | 0.292        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.3        |\n",
      "|    explained_variance   | 0.0765       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.87         |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    reward               | -0.008197777 |\n",
      "|    std                  | 1.2          |\n",
      "|    value_loss           | 31.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1792        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022962091 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.4       |\n",
      "|    explained_variance   | 0.0302      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 50          |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00797    |\n",
      "|    reward               | -0.55041814 |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1812        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030306041 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.4       |\n",
      "|    explained_variance   | -0.0133     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 67.8        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.000764   |\n",
      "|    reward               | 4.9244113   |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 206         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1832        |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028664932 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.4       |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | 0.0019      |\n",
      "|    reward               | 0.21847676  |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 48.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1853        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027408203 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.5       |\n",
      "|    explained_variance   | 0.00158     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.9        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00807    |\n",
      "|    reward               | 0.6061848   |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 90.7        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 4: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5060161.14\n",
      "total_reward: 4060161.14\n",
      "total_cost: 183863.98\n",
      "total_trades: 58679\n",
      "Sharpe: 0.848\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 45        |\n",
      "|    time_elapsed    | 252       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 325       |\n",
      "|    critic_loss     | 189       |\n",
      "|    ent_coef        | 0.108     |\n",
      "|    ent_coef_loss   | -105      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 13.078064 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 45        |\n",
      "|    time_elapsed    | 506       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 131       |\n",
      "|    critic_loss     | 933       |\n",
      "|    ent_coef        | 0.0342    |\n",
      "|    ent_coef_loss   | -153      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 23043     |\n",
      "|    reward          | 18.951672 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7178408.36\n",
      "total_reward: 6178408.36\n",
      "total_cost: 9118.39\n",
      "total_trades: 54645\n",
      "Sharpe: 0.909\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 45        |\n",
      "|    time_elapsed    | 759       |\n",
      "|    total_timesteps | 34716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 65.3      |\n",
      "|    critic_loss     | 30.1      |\n",
      "|    ent_coef        | 0.011     |\n",
      "|    ent_coef_loss   | -169      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 34615     |\n",
      "|    reward          | -3.360691 |\n",
      "----------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 16          |\n",
      "|    fps             | 45          |\n",
      "|    time_elapsed    | 1013        |\n",
      "|    total_timesteps | 46288       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | 42          |\n",
      "|    critic_loss     | 37.3        |\n",
      "|    ent_coef        | 0.00367     |\n",
      "|    ent_coef_loss   | -102        |\n",
      "|    learning_rate   | 0.0001      |\n",
      "|    n_updates       | 46187       |\n",
      "|    reward          | -0.06650945 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 1266     |\n",
      "|    total_timesteps | 57860    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.8     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    ent_coef        | 0.00209  |\n",
      "|    ent_coef_loss   | 3.27     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 57759    |\n",
      "|    reward          | 2.456344 |\n",
      "---------------------------------\n",
      "day: 2892, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6606023.19\n",
      "total_reward: 5606023.19\n",
      "total_cost: 2610.69\n",
      "total_trades: 43440\n",
      "Sharpe: 0.934\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 45        |\n",
      "|    time_elapsed    | 1522      |\n",
      "|    total_timesteps | 69432     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 26.4      |\n",
      "|    critic_loss     | 7.65      |\n",
      "|    ent_coef        | 0.00185   |\n",
      "|    ent_coef_loss   | -0.801    |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 69331     |\n",
      "|    reward          | 2.1811397 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
